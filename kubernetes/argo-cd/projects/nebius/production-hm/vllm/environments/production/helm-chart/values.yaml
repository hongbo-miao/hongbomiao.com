# https://github.com/vllm-project/production-stack/blob/main/helm/values.yaml
---
servingEngineSpec:
  enableEngine: true
  runtimeClassName: nebius-nvidia
  tolerations:
    - key: environment
      operator: Equal
      value: production
      effect: NoSchedule
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  vllmApiKey:
    secretName: hm-vllm-secret
    secretKey: VLLM_API_KEY
  modelSpec:
    - name: qwen3-0-6b
      repository: harbor.hongbomiao.com/docker-hub-proxy-cache/lmcache/vllm-openai
      # https://hub.docker.com/r/lmcache/vllm-openai/tags
      tag: v0.3.14
      modelURL: Qwen/Qwen3-0.6B
      replicaCount: 3
      requestCPU: 1
      requestMemory: 4200Mi
      limitMemory: 4200Mi
      requestGPU: 1
      requestGPUType: nvidia.com/mig-2g.35gb
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/mig.config
                    operator: In
                    values:
                      - all-2g.35gb
                  - key: environment
                    operator: In
                    values:
                      - production
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: model
                    operator: In
                    values:
                      - qwen3-0-6b
              topologyKey: kubernetes.io/hostname
      vllmConfig:
        enablePrefixCaching: true
        enableChunkedPrefill: true
        maxModelLen: 8192
        dtype: auto
        tensorParallelSize: 1
    - name: qwen3-0-6b-20260101
      repository: harbor.hongbomiao.com/docker-hub-proxy-cache/lmcache/vllm-openai
      # https://hub.docker.com/r/lmcache/vllm-openai/tags
      tag: v0.3.14
      modelURL: s3://production-hm-models/qwen3-0-6b-20260101
      replicaCount: 3
      requestCPU: 1
      requestMemory: 4200Mi
      limitMemory: 4200Mi
      requestGPU: 1
      requestGPUType: nvidia.com/mig-2g.35gb
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/mig.config
                    operator: In
                    values:
                      - all-2g.35gb
                  - key: environment
                    operator: In
                    values:
                      - production
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: model
                    operator: In
                    values:
                      - qwen3-0-6b-20260101
              topologyKey: kubernetes.io/hostname
      vllmConfig:
        enablePrefixCaching: true
        enableChunkedPrefill: true
        maxModelLen: 8192
        dtype: auto
        tensorParallelSize: 1
        extraArgs:
          - "--served-model-name=qwen3-0-6b-20260101"
      env:
        - name: AWS_ENDPOINT_URL
          value: https://storage.us-central1.nebius.cloud
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: hm-vllm-secret
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: hm-vllm-secret
              key: AWS_SECRET_ACCESS_KEY
  startupProbe:
    initialDelaySeconds: 15
    periodSeconds: 10
    failureThreshold: 360
    httpGet:
      path: /health
      port: 8000
routerSpec:
  enableRouter: true
  repository: harbor.hongbomiao.com/docker-hub-proxy-cache/lmcache/lmstack-router
  # https://hub.docker.com/r/lmcache/lmstack-router/tags
  tag: v0.1.10
  replicaCount: 3
  serviceDiscovery: k8s
  routingLogic: roundrobin
  nodeSelector:
    environment: production
  tolerations:
    - key: environment
      operator: Equal
      value: production
      effect: NoSchedule
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                  - router
          topologyKey: kubernetes.io/hostname
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  startupProbe:
    initialDelaySeconds: 15
    periodSeconds: 10
    failureThreshold: 60
    httpGet:
      path: /health
      port: 8000
  resources:
    requests:
      cpu: 500m
      memory: 500Mi
    limits:
      memory: 1Gi
