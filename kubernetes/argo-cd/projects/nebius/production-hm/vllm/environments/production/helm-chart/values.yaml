# https://github.com/vllm-project/production-stack/blob/main/helm/values.yaml
---
servingEngineSpec:
  enableEngine: true
  runtimeClassName: nebius-nvidia
  vllmApiKey:
    secretName: hm-vllm-secret
    secretKey: VLLM_API_KEY
  modelSpec:
    - name: qwen3-0-6b
      repository: harbor.hongbomiao.com/docker-hub-proxy-cache/lmcache/vllm-openai
      # https://hub.docker.com/r/lmcache/vllm-openai/tags
      tag: v0.3.13
      modelURL: Qwen/Qwen3-0.6B
      replicaCount: 1
      requestCPU: 1
      requestMemory: 4200Mi
      limitMemory: 4200Mi
      requestGPU: 1
      requestGPUType: nvidia.com/mig-2g.35gb
      nodeSelector:
        nvidia.com/mig.config: all-2g.35gb
      vllmConfig:
        enablePrefixCaching: true
        enableChunkedPrefill: true
        maxModelLen: 8192
        dtype: auto
        tensorParallelSize: 1
    - name: qwen3-0-6b-20260101
      repository: harbor.hongbomiao.com/docker-hub-proxy-cache/lmcache/vllm-openai
      # https://hub.docker.com/r/lmcache/vllm-openai/tags
      tag: v0.3.13
      modelURL: s3://production-hm-models/qwen3-0-6b-20260101
      replicaCount: 1
      requestCPU: 1
      requestMemory: 4200Mi
      limitMemory: 4200Mi
      requestGPU: 1
      requestGPUType: nvidia.com/mig-2g.35gb
      nodeSelector:
        nvidia.com/mig.config: all-2g.35gb
      vllmConfig:
        enablePrefixCaching: true
        enableChunkedPrefill: true
        maxModelLen: 8192
        dtype: auto
        tensorParallelSize: 1
        extraArgs:
          - "--served-model-name=qwen3-0-6b-20260101"
      env:
        - name: AWS_ENDPOINT_URL
          value: https://storage.us-central1.nebius.cloud
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: hm-vllm-secret
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: hm-vllm-secret
              key: AWS_SECRET_ACCESS_KEY
  startupProbe:
    initialDelaySeconds: 15
    periodSeconds: 10
    failureThreshold: 360
    httpGet:
      path: /health
      port: 8000
routerSpec:
  enableRouter: true
  repository: harbor.hongbomiao.com/docker-hub-proxy-cache/lmcache/lmstack-router
  # https://hub.docker.com/r/lmcache/lmstack-router/tags
  tag: 0.1.10.dev3-g9753b1979.d20260203
  replicaCount: 3
  serviceDiscovery: k8s
  routingLogic: roundrobin
  startupProbe:
    initialDelaySeconds: 15
    periodSeconds: 10
    failureThreshold: 60
    httpGet:
      path: /health
      port: 8000
  resources:
    requests:
      cpu: 500m
      memory: 500Mi
    limits:
      memory: 1Gi
