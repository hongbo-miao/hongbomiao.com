---
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: hm-ray-cluster
  namespace: production-hm-ray-cluster
  labels:
    app.kubernetes.io/name: hm-ray-cluster-deployment
    app.kubernetes.io/part-of: production-hm-ray-cluster
spec:
  rayVersion: 2.43.0
  # https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-cluster.autoscaler-v2.yaml
  enableInTreeAutoscaling: true
  autoscalerOptions:
    upscalingMode: Default
    idleTimeoutSeconds: 60
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
  # https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-cluster.external-redis.yaml
  gcsFaultToleranceOptions:
    redisAddress: redis://hm-ray-cluster-valkey-primary.production-hm-ray-cluster-valkey.svc:6379
    redisPassword:
      valueFrom:
        secretKeyRef:
          name: hm-ray-cluster-secret
          key: VALKEY_PASSWORD
  headGroupSpec:
    rayStartParams:
      num-cpus: "0"
    template:
      spec:
        priorityClassName: high
        serviceAccountName: hm-ray-cluster-service-account
        # https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-cluster.autoscaler-v2.yaml
        restartPolicy: Never
        containers:
          - name: ray-head
            image: harbor.hongbomiao.com/docker-hub-proxy-cache/rayproject/ray:2.43.0-py312-cpu
            # https://docs.ray.io/en/latest/cluster/kubernetes/k8s-ecosystem/pyspy.html
            securityContext:
              capabilities:
                add:
                  - SYS_PTRACE
            ports:
              - containerPort: 6379
                name: gcs
              - containerPort: 8265
                name: dashboard
              - containerPort: 10001
                name: client
              - containerPort: 8000
                name: serve
            env:
              # https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-cluster.autoscaler-v2.yaml
              - name: RAY_enable_autoscaler_v2
                value: "1"
              # https://docs.ray.io/en/latest/cluster/configure-manage-dashboard.html#embedding-grafana-visualizations-into-ray-dashboard
              - name: RAY_GRAFANA_IFRAME_HOST
                value: https://grafana.hongbomiao.com
              - name: RAY_GRAFANA_HOST
                value: http://hm-grafana.production-hm-grafana.svc:80
              - name: RAY_PROMETHEUS_HOST
                value: http://hm-prometheus-kube-pr-prometheus.production-hm-prometheus.svc:9090
              - name: RAY_PROMETHEUS_NAME
                value: hm-prometheus
            resources:
              requests:
                cpu: 1000m
                memory: 2Gi
              limits:
                cpu: 2000m
                memory: 4Gi
            volumeMounts:
              - mountPath: /tmp/ray
                name: ray-logs
              - mountPath: /etc/alloy/config.alloy
                subPath: config.alloy
                name: alloy-config
          - name: alloy
            image: harbor.hongbomiao.com/docker-hub-proxy-cache/grafana/alloy:v1.6.1
            env:
              - name: NODE_TYPE
                value: worker
              - name: LOKI_URL
                value: http://hm-loki-gateway.production-hm-loki.svc:80/loki/api/v1/push
            resources:
              requests:
                cpu: 100m
                memory: 128Mi
              limits:
                cpu: 200m
                memory: 256Mi
            volumeMounts:
              - mountPath: /tmp/ray
                name: ray-logs
              - mountPath: /etc/alloy/config.alloy
                subPath: config.alloy
                name: alloy-config
        volumes:
          - name: ray-logs
            emptyDir: {}
          - name: alloy-config
            configMap:
              name: hm-ray-cluster-config-map
  workerGroupSpecs:
    - groupName: 1g
      replicas: 1
      minReplicas: 1
      maxReplicas: 20
      rayStartParams: {}
      template:
        spec:
          serviceAccountName: hm-ray-cluster-service-account
          restartPolicy: Never
          containers:
            - name: ray-worker
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/rayproject/ray:2.43.0-py312-cpu
              securityContext:
                capabilities:
                  add:
                    - SYS_PTRACE
              resources:
                requests:
                  cpu: 1000m
                  memory: 1Gi
                limits:
                  cpu: 1000m
                  memory: 1Gi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
            - name: alloy
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/grafana/alloy:v1.6.1
              env:
                - name: NODE_TYPE
                  value: worker
                - name: LOKI_URL
                  value: http://hm-loki-gateway.production-hm-loki.svc:80/loki/api/v1/push
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 200m
                  memory: 256Mi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /etc/alloy/config.alloy
                  subPath: config.alloy
                  name: alloy-config
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: alloy-config
              configMap:
                name: hm-ray-cluster-config-map
    - groupName: 2g
      replicas: 1
      minReplicas: 1
      maxReplicas: 20
      rayStartParams: {}
      template:
        spec:
          serviceAccountName: hm-ray-cluster-service-account
          restartPolicy: Never
          containers:
            - name: ray-worker
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/rayproject/ray:2.43.0-py312-cpu
              securityContext:
                capabilities:
                  add:
                    - SYS_PTRACE
              resources:
                requests:
                  cpu: 2000m
                  memory: 2Gi
                limits:
                  cpu: 2000m
                  memory: 2Gi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
            - name: alloy
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/grafana/alloy:v1.6.1
              env:
                - name: NODE_TYPE
                  value: worker
                - name: LOKI_URL
                  value: http://hm-loki-gateway.production-hm-loki.svc:80/loki/api/v1/push
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 200m
                  memory: 256Mi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /etc/alloy/config.alloy
                  subPath: config.alloy
                  name: alloy-config
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: alloy-config
              configMap:
                name: hm-ray-cluster-config-map
    - groupName: 4g
      replicas: 1
      minReplicas: 1
      maxReplicas: 15
      rayStartParams: {}
      template:
        spec:
          serviceAccountName: hm-ray-cluster-service-account
          restartPolicy: Never
          containers:
            - name: ray-worker
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/rayproject/ray:2.43.0-py312-cpu
              securityContext:
                capabilities:
                  add:
                    - SYS_PTRACE
              resources:
                requests:
                  cpu: 4000m
                  memory: 4Gi
                limits:
                  cpu: 4000m
                  memory: 4Gi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
            - name: alloy
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/grafana/alloy:v1.6.1
              env:
                - name: NODE_TYPE
                  value: worker
                - name: LOKI_URL
                  value: http://hm-loki-gateway.production-hm-loki.svc:80/loki/api/v1/push
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 200m
                  memory: 256Mi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /etc/alloy/config.alloy
                  subPath: config.alloy
                  name: alloy-config
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: alloy-config
              configMap:
                name: hm-ray-cluster-config-map
    - groupName: 8g
      replicas: 1
      minReplicas: 1
      maxReplicas: 10
      rayStartParams: {}
      template:
        spec:
          serviceAccountName: hm-ray-cluster-service-account
          restartPolicy: Never
          containers:
            - name: ray-worker
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/rayproject/ray:2.43.0-py312-cpu
              securityContext:
                capabilities:
                  add:
                    - SYS_PTRACE
              resources:
                requests:
                  cpu: 6000m
                  memory: 8Gi
                limits:
                  cpu: 6000m
                  memory: 8Gi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
            - name: alloy
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/grafana/alloy:v1.6.1
              env:
                - name: NODE_TYPE
                  value: worker
                - name: LOKI_URL
                  value: http://hm-loki-gateway.production-hm-loki.svc:80/loki/api/v1/push
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 200m
                  memory: 256Mi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /etc/alloy/config.alloy
                  subPath: config.alloy
                  name: alloy-config
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: alloy-config
              configMap:
                name: hm-ray-cluster-config-map
    - groupName: 16g
      replicas: 0
      minReplicas: 0
      maxReplicas: 8
      rayStartParams: {}
      template:
        spec:
          serviceAccountName: hm-ray-cluster-service-account
          restartPolicy: Never
          containers:
            - name: ray-worker
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/rayproject/ray:2.43.0-py312-cpu
              securityContext:
                capabilities:
                  add:
                    - SYS_PTRACE
              resources:
                requests:
                  cpu: 8000m
                  memory: 16Gi
                limits:
                  cpu: 8000m
                  memory: 16Gi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
            - name: alloy
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/grafana/alloy:v1.6.1
              env:
                - name: NODE_TYPE
                  value: worker
                - name: LOKI_URL
                  value: http://hm-loki-gateway.production-hm-loki.svc:80/loki/api/v1/push
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 200m
                  memory: 256Mi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /etc/alloy/config.alloy
                  subPath: config.alloy
                  name: alloy-config
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: alloy-config
              configMap:
                name: hm-ray-cluster-config-map
    - groupName: 28g
      replicas: 0
      minReplicas: 0
      maxReplicas: 5
      rayStartParams: {}
      template:
        spec:
          priorityClassName: medium
          serviceAccountName: hm-ray-cluster-service-account
          restartPolicy: Never
          containers:
            - name: ray-worker
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/rayproject/ray:2.43.0-py312-cpu
              securityContext:
                capabilities:
                  add:
                    - SYS_PTRACE
              resources:
                requests:
                  cpu: 12000m
                  memory: 28Gi
                limits:
                  cpu: 12000m
                  memory: 28Gi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
            - name: alloy
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/grafana/alloy:v1.6.1
              env:
                - name: NODE_TYPE
                  value: worker
                - name: LOKI_URL
                  value: http://hm-loki-gateway.production-hm-loki.svc:80/loki/api/v1/push
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 200m
                  memory: 256Mi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /etc/alloy/config.alloy
                  subPath: config.alloy
                  name: alloy-config
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: alloy-config
              configMap:
                name: hm-ray-cluster-config-map
    - groupName: 58g
      replicas: 0
      minReplicas: 0
      maxReplicas: 5
      rayStartParams: {}
      template:
        spec:
          priorityClassName: medium
          serviceAccountName: hm-ray-cluster-service-account
          restartPolicy: Never
          containers:
            - name: ray-worker
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/rayproject/ray:2.43.0-py312-cpu
              securityContext:
                capabilities:
                  add:
                    - SYS_PTRACE
              resources:
                requests:
                  cpu: 15000m
                  memory: 58Gi
                limits:
                  cpu: 15000m
                  memory: 58Gi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
            - name: alloy
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/grafana/alloy:v1.6.1
              env:
                - name: NODE_TYPE
                  value: worker
                - name: LOKI_URL
                  value: http://hm-loki-gateway.production-hm-loki.svc:80/loki/api/v1/push
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 200m
                  memory: 256Mi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /etc/alloy/config.alloy
                  subPath: config.alloy
                  name: alloy-config
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: alloy-config
              configMap:
                name: hm-ray-cluster-config-map
    - groupName: 460g
      replicas: 0
      minReplicas: 0
      maxReplicas: 3
      rayStartParams: {}
      template:
        spec:
          priorityClassName: medium
          serviceAccountName: hm-ray-cluster-service-account
          restartPolicy: Never
          containers:
            - name: ray-worker
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/rayproject/ray:2.43.0-py312-cpu
              securityContext:
                capabilities:
                  add:
                    - SYS_PTRACE
              resources:
                requests:
                  cpu: 57000m
                  memory: 460Gi
                limits:
                  cpu: 57000m
                  memory: 460Gi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
            - name: alloy
              image: harbor.hongbomiao.com/docker-hub-proxy-cache/grafana/alloy:v1.6.1
              env:
                - name: NODE_TYPE
                  value: worker
                - name: LOKI_URL
                  value: http://hm-loki-gateway.production-hm-loki.svc:80/loki/api/v1/push
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 200m
                  memory: 256Mi
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
                - mountPath: /etc/alloy/config.alloy
                  subPath: config.alloy
                  name: alloy-config
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: alloy-config
              configMap:
                name: hm-ray-cluster-config-map
