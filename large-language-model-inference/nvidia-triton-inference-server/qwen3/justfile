NVIDIA_TRITON_INFERENCE_SERVER_URL := "http://localhost:8000"
MODEL := "Qwen/Qwen3-0.6B"

run:
    docker run --gpus=all \
        --shm-size=32g \
        --volume="$HOME/.cache/huggingface:/root/.cache/huggingface" \
        --publish=8000:8000 \
        --ipc=host \
        nvcr.io/nvidia/tritonserver:25.12-vllm-python-py3 \
            python3 -m vllm.entrypoints.openai.api_server \
                --host=0.0.0.0 \
                --port=8000 \
                --tensor-parallel-size=2 \
                --gpu-memory-utilization=0.9 \
                --model={{ MODEL }} \
                --max-num-seqs=128 \
                --max-model-len=8192

query:
    curl {{ NVIDIA_TRITON_INFERENCE_SERVER_URL }}/v1/chat/completions \
        --header "Content-Type: application/json" \
        --data '{ \
            "model": "{{ MODEL }}", \
            "messages": [ \
                {"role": "system", "content": "You are a helpful assistant."}, \
                {"role": "user", "content": "What is gravity?"} \
            ] \
        }'
