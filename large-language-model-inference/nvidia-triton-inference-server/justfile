NVIDIA_TRITON_INFERENCE_SERVER_URL := "http://localhost:8000"
MODEL := "Qwen/Qwen2.5-0.5B-Instruct"

run:
    docker run --gpus=all \
        --volume="$HOME/.cache/huggingface:/root/.cache/huggingface" \
        --publish=8000:8000 \
        nvcr.io/nvidia/tritonserver:25.05-vllm-python-py3 \
            python3 -m vllm.entrypoints.openai.api_server \
                --port=8000 \
                --tensor-parallel-size=2 \
                --gpu-memory-utilization=0.9 \
                --model={{ MODEL }} \
                --max_num_seqs=128 \
                --max_model_len=8192

query:
    curl {{ NVIDIA_TRITON_INFERENCE_SERVER_URL }}/v1/chat/completions \
        --header "Content-Type: application/json" \
        --data '{ \
            "model": "{{ MODEL }}", \
            "messages": [ \
                {"role": "system", "content": "You are a helpful assistant."}, \
                {"role": "user", "content": "Tell me a joke."} \
            ] \
        }'
