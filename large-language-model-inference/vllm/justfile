VLLM_URL := "http://localhost:8000"
AUDIO_PATH := "data/audio.wav"

run:
    docker run \
        --gpus=all \
        --ipc=host \
        --volume="$HOME/.cache/huggingface:/root/.cache/huggingface" \
        --publish=8000:8000 \
        --entrypoint=/bin/bash \
        vllm/vllm-openai:v0.13.0 \
            -c "pip install vllm[audio] && python3 -m vllm.entrypoints.openai.api_server \
                --model=cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit \
                --limit-mm-per-prompt='{\"audio\":1}' \
                --max-model-len=4096 \
                --gpu-memory-utilization=0.9 \
                --max-num-seqs=1"

query:
    #!/usr/bin/env bash
    cat <<EOF | curl "{{ VLLM_URL }}/v1/chat/completions" \
        --header "Content-Type: application/json" \
        --data @-
    {
        "model": "cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit",
        "messages": [
            {
                "role": "user",
                "content": [
                    { "type": "text", "text": "What is in this audio?" },
                    {
                        "type": "input_audio",
                        "input_audio": {
                            "data": "$(base64 < "{{ AUDIO_PATH }}" | tr -d '\n')",
                            "format": "wav"
                        }
                    }
                ]
            }
        ]
    }
    EOF
