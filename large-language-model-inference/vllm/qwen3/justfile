VLLM_URL := "http://localhost:8000"
MODEL := "Qwen/Qwen3-0.6B"

run:
    docker run \
        --gpus=all \
        --shm-size=32g \
        --volume="$HOME/.cache/huggingface:/root/.cache/huggingface" \
        --publish=0.0.0.0:8000:8000 \
        --ipc=host \
        docker.io/vllm/vllm-openai:v0.13.0 \
            --host=0.0.0.0 \
            --port=8000 \
            --tensor-parallel-size=2 \
            --gpu-memory-utilization=0.9 \
            --model={{ MODEL }} \
            --max-num-seqs=128 \
            --max-model-len=8192

query:
    curl {{ VLLM_URL }}/v1/chat/completions \
        --header "Content-Type: application/json" \
        --data '{ \
            "model": "{{ MODEL }}", \
            "messages": [ \
                {"role": "system", "content": "You are a helpful assistant."}, \
                {"role": "user", "content": "What is gravity?"} \
            ] \
        }'
