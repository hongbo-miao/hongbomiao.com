---
name: . Test Data Processing

'on':
  workflow_call:
    inputs:
      # Workflows
      workflow:
        required: true
        type: boolean
      # Applications
      flink-stream-tweets:
        required: true
        type: boolean
      hm-cocoindex-embed-markdown-to-postgres:
        required: true
        type: boolean
      hm-pathway-aggregate-hourly-events:
        required: true
        type: boolean
      hm-spark-analyze-coffee-customers:
        required: true
        type: boolean
      hm-spark-find-retired-people-python:
        required: true
        type: boolean
      hm-spark-find-retired-people-scala:
        required: true
        type: boolean
      hm-spark-find-taxi-top-routes-sql:
        required: true
        type: boolean
      hm-spark-find-taxi-top-routes:
        required: true
        type: boolean
      hm-spark-ingest-from-s3-to-kafka:
        required: true
        type: boolean
      hm-spark-recommend-movies:
        required: true
        type: boolean
      ingest-flac-to-parquet:
        required: true
        type: boolean
      kafka-rust-proto-consumer:
        required: true
        type: boolean
      kafka-rust-proto-producer:
        required: true
        type: boolean
      kafka-rust-udp-kafka-bridge:
        required: true
        type: boolean
      kafka-rust-zeromq-kafka-bridge:
        required: true
        type: boolean
      nats-audio-file-publisher:
        required: true
        type: boolean
      nats-audio-file-subscriber:
        required: true
        type: boolean
      nats-audio-stream-publisher:
        required: true
        type: boolean
      nats-audio-stream-transcriber:
        required: true
        type: boolean
      nats-postgres-bridge:
        required: true
        type: boolean
      nats-telemetry-stream-publisher:
        required: true
        type: boolean
      nats-telemetry-stream-subscriber:
        required: true
        type: boolean

jobs:
  ingest-flac-to-parquet-test:
    name: ingest-flac-to-parquet | Test
    if: ${{ inputs.workflow || inputs.hm-ingest-flac-to-parquet }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/ingest-flac-to-parquet/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/ingest-flac-to-parquet/.python-version
      - name: Install dependencies
        working-directory: data-processing/ingest-flac-to-parquet
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/ingest-flac-to-parquet
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/ingest-flac-to-parquet

  cocoindex-embed-markdown-to-postgres-test:
    name: CocoIndex (embed-markdown-to-postgres) | Test
    if: ${{ inputs.workflow || inputs.hm-cocoindex-embed-markdown-to-postgres }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/hm-cocoindex/flows/embed-markdown-to-postgres/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/hm-cocoindex/flows/embed-markdown-to-postgres/.python-version
      - name: Install dependencies
        working-directory: data-processing/hm-cocoindex/flows/embed-markdown-to-postgres
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-cocoindex/flows/embed-markdown-to-postgres
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/hm-cocoindex/flows/embed-markdown-to-postgres

  pathway-aggregate-hourly-events-test:
    name: Pathway (aggregate-hourly-events) | Test
    if: ${{ inputs.workflow || inputs.hm-pathway-aggregate-hourly-events }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/hm-pathway/pipelines/aggregate-hourly-events/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/hm-pathway/pipelines/aggregate-hourly-events/.python-version
      - name: Install dependencies
        working-directory: data-processing/hm-pathway/pipelines/aggregate-hourly-events
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-pathway/pipelines/aggregate-hourly-events
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/hm-pathway/pipelines/aggregate-hourly-events

  spark-analyze-coffee-customers-test:
    name: Spark (analyze-coffee-customers) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-analyze-coffee-customers }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/analyze-coffee-customers/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/hm-spark/applications/analyze-coffee-customers/.python-version
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/analyze-coffee-customers
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/analyze-coffee-customers
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/hm-spark/applications/analyze-coffee-customers

  spark-find-retired-people-python-test:
    name: Spark (find-retired-people-python) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-find-retired-people-python }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/find-retired-people-python/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/hm-spark/applications/find-retired-people-python/.python-version
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/find-retired-people-python
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-retired-people-python
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/hm-spark/applications/find-retired-people-python

  spark-find-retired-people-scala-test:
    name: Spark (find-retired-people-scala) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-find-retired-people-scala }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Set up Java
        uses: actions/setup-java@v5.0.0
        with:
          distribution: corretto
          cache: sbt
          java-version-file: data-processing/hm-spark/applications/find-retired-people-scala/.java-version
      - name: Set up sbt
        uses: sbt/setup-sbt@v1.1.14
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-retired-people-scala
        run: |
          sbt test

  spark-find-taxi-top-routes-test:
    name: Spark (find-taxi-top-routes) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-find-taxi-top-routes }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/find-taxi-top-routes/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/hm-spark/applications/find-taxi-top-routes/.python-version
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/hm-spark/applications/find-taxi-top-routes

  spark-find-taxi-top-routes-sql-test:
    name: Spark (find-taxi-top-routes-sql) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-find-taxi-top-routes-sql }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/find-taxi-top-routes-sql/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/hm-spark/applications/find-taxi-top-routes-sql/.python-version
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes-sql
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes-sql
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/hm-spark/applications/find-taxi-top-routes-sql

  spark-ingest-from-s3-to-kafka-test:
    name: Spark (ingest-from-s3-to-kafka) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-ingest-from-s3-to-kafka }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Set up Java
        uses: actions/setup-java@v5.0.0
        with:
          distribution: corretto
          cache: sbt
          java-version-file: data-processing/hm-spark/applications/ingest-from-s3-to-kafka/.java-version
      - name: Set up sbt
        uses: sbt/setup-sbt@v1.1.14
      - name: Test
        working-directory: data-processing/hm-spark/applications/ingest-from-s3-to-kafka
        run: |
          sbt test

  spark-recommend-movies-test:
    name: Spark (recommend-movies) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-recommend-movies }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/recommend-movies/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/hm-spark/applications/recommend-movies/.python-version
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/recommend-movies
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/recommend-movies
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/hm-spark/applications/recommend-movies

  flink-stream-tweets-test:
    name: Flink (stream-tweets) | Test
    if: ${{ inputs.workflow || inputs.flink-stream-tweets }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Set up Java
        uses: actions/setup-java@v5.0.0
        with:
          distribution: corretto
          cache: maven
          java-version-file: data-processing/flink/applications/stream-tweets/.java-version
      - name: Test
        working-directory: data-processing/flink/applications/stream-tweets
        run: |
          mvn test

  kafka-rust-proto-consumer-test:
    name: Kafka Rust (proto-consumer) | Test
    if: ${{ inputs.workflow || inputs.kafka-rust-proto-consumer }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      # librdkafka is for rdkafka
      - name: Install librdkafka
        env:
          LIBRDKAFKA_VERSION: 2.11.1
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz "https://github.com/edenhill/librdkafka/archive/refs/tags/v${LIBRDKAFKA_VERSION}.tar.gz"
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd "librdkafka-${LIBRDKAFKA_VERSION}"
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f "librdkafka-${LIBRDKAFKA_VERSION}/"
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.15.2
        with:
          cache: true
          rust-src-dir: data-processing/kafka/kafka-client/kafka-rust/proto-consumer
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-consumer
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-consumer
        run: |
          cargo test --all-features

  kafka-rust-proto-producer-test:
    name: Kafka Rust (proto-producer) | Test
    if: ${{ inputs.workflow || inputs.kafka-rust-proto-producer }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      # librdkafka is for rdkafka
      - name: Install librdkafka
        env:
          LIBRDKAFKA_VERSION: 2.11.1
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz "https://github.com/edenhill/librdkafka/archive/refs/tags/v${LIBRDKAFKA_VERSION}.tar.gz"
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd "librdkafka-${LIBRDKAFKA_VERSION}"
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f "librdkafka-${LIBRDKAFKA_VERSION}/"
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.15.2
        with:
          cache: true
          rust-src-dir: data-processing/kafka/kafka-client/kafka-rust/proto-producer
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-producer
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-producer
        run: |
          cargo test --all-features

  kafka-rust-udp-kafka-bridge-test:
    name: Kafka Rust (udp-kafka-bridge) | Test
    if: ${{ inputs.workflow || inputs.kafka-rust-udp-kafka-bridge }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      # librdkafka is for rdkafka
      - name: Install librdkafka
        env:
          LIBRDKAFKA_VERSION: 2.11.1
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz "https://github.com/edenhill/librdkafka/archive/refs/tags/v${LIBRDKAFKA_VERSION}.tar.gz"
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd "librdkafka-${LIBRDKAFKA_VERSION}"
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f "librdkafka-${LIBRDKAFKA_VERSION}/"
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.15.2
        with:
          cache: true
          rust-src-dir: data-processing/kafka/kafka-client/kafka-rust/udp-kafka-bridge
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/udp-kafka-bridge
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/udp-kafka-bridge
        run: |
          cargo test --all-features

  kafka-rust-zeromq-kafka-bridge-test:
    name: Kafka Rust (zeromq-kafka-bridge) | Test
    if: ${{ inputs.workflow || inputs.kafka-rust-zeromq-kafka-bridge }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      # librdkafka is for rdkafka
      - name: Install librdkafka
        env:
          LIBRDKAFKA_VERSION: 2.11.1
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz "https://github.com/edenhill/librdkafka/archive/refs/tags/v${LIBRDKAFKA_VERSION}.tar.gz"
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd "librdkafka-${LIBRDKAFKA_VERSION}"
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f "librdkafka-${LIBRDKAFKA_VERSION}/"
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.15.2
        with:
          cache: true
          rust-src-dir: data-processing/kafka/kafka-client/kafka-rust/zeromq-kafka-bridge
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/zeromq-kafka-bridge
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/zeromq-kafka-bridge
        run: |
          cargo test --all-features

  nats-audio-file-publisher-test:
    name: NATS (audio-file-publisher) | Test
    if: ${{ inputs.workflow || inputs.nats-audio-file-publisher }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/nats/audio-file-publisher/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/nats/audio-file-publisher/.python-version
      - name: Install dependencies
        working-directory: data-processing/nats/audio-file-publisher
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/nats/audio-file-publisher
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/nats/audio-file-publisher

  nats-audio-file-subscriber-test:
    name: NATS (audio-file-subscriber) | Test
    if: ${{ inputs.workflow || inputs.nats-audio-file-subscriber }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/nats/audio-file-subscriber/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/nats/audio-file-subscriber/.python-version
      - name: Install dependencies
        working-directory: data-processing/nats/audio-file-subscriber
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/nats/audio-file-subscriber
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/nats/audio-file-subscriber

  nats-audio-stream-publisher-test:
    name: NATS (audio-stream-publisher) | Test
    if: ${{ inputs.workflow || inputs.nats-audio-stream-publisher }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.15.2
        with:
          cache: true
          rust-src-dir: data-processing/nats/audio-stream-publisher
      - name: Install dependencies
        working-directory: data-processing/nats/audio-stream-publisher
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/nats/audio-stream-publisher
        run: |
          cargo test --all-features

  nats-audio-stream-transcriber-test:
    name: NATS (audio-stream-transcriber) | Test
    if: ${{ inputs.workflow || inputs.nats-audio-stream-transcriber }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.15.2
        with:
          cache: true
          rust-src-dir: data-processing/nats/audio-stream-transcriber
      - name: Install system dependencies
        uses: awalsh128/cache-apt-pkgs-action@v1.6.0
        with:
          # capnproto is for capnp
          packages: capnproto
          version: 1.0
      - name: Install dependencies
        working-directory: data-processing/nats/audio-stream-transcriber
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/nats/audio-stream-transcriber
        run: |
          cargo test --all-features

  nats-postgres-bridge-test:
    name: NATS (nats-postgres-bridge) | Test
    if: ${{ inputs.workflow || inputs.nats-postgres-bridge }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.15.2
        with:
          cache: true
          rust-src-dir: data-processing/nats/nats-postgres-bridge
      - name: Install system dependencies
        uses: awalsh128/cache-apt-pkgs-action@v1.6.0
        with:
          # capnproto is for capnp
          packages: capnproto
          version: 1.0
      - name: Install dependencies
        working-directory: data-processing/nats/nats-postgres-bridge
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/nats/nats-postgres-bridge
        run: |
          cargo test --all-features

  nats-telemetry-stream-publisher-test:
    name: NATS (telemetry-stream-publisher) | Test
    if: ${{ inputs.workflow || inputs.nats-telemetry-stream-publisher }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/nats/telemetry-stream-publisher/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/nats/telemetry-stream-publisher/.python-version
      - name: Install dependencies
        working-directory: data-processing/nats/telemetry-stream-publisher
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/nats/telemetry-stream-publisher
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/nats/telemetry-stream-publisher

  nats-telemetry-stream-subscriber-test:
    name: NATS (telemetry-stream-subscriber) | Test
    if: ${{ inputs.workflow || inputs.nats-telemetry-stream-subscriber }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v6.0.1
      - name: Install uv
        uses: astral-sh/setup-uv@v7.1.4
        with:
          version: 0.9.15
          enable-cache: true
          cache-dependency-glob: data-processing/nats/telemetry-stream-subscriber/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v6.1.0
        with:
          python-version-file: data-processing/nats/telemetry-stream-subscriber/.python-version
      - name: Install dependencies
        working-directory: data-processing/nats/telemetry-stream-subscriber
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/nats/telemetry-stream-subscriber
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.5.1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: hongbo-miao/hongbomiao.com
          directory: data-processing/nats/telemetry-stream-subscriber
