---
name: . Test Data Processing

'on':
  workflow_call:
    inputs:
      # Workflows
      workflow:
        required: true
        type: boolean
      # Applications
      flink-stream-tweets:
        required: true
        type: boolean
      hm-spark-analyze-coffee-customers:
        required: true
        type: boolean
      hm-spark-find-retired-people-python:
        required: true
        type: boolean
      hm-spark-find-retired-people-scala:
        required: true
        type: boolean
      hm-spark-find-taxi-top-routes-sql:
        required: true
        type: boolean
      hm-spark-find-taxi-top-routes:
        required: true
        type: boolean
      hm-spark-ingest-from-s3-to-kafka:
        required: true
        type: boolean
      hm-spark-recommend-movies:
        required: true
        type: boolean
      kafka-rust-proto-consumer:
        required: true
        type: boolean
      kafka-rust-proto-producer:
        required: true
        type: boolean
      kafka-rust-udp-kafka-bridge:
        required: true
        type: boolean
      kafka-rust-zeromq-kafka-bridge:
        required: true
        type: boolean

jobs:
  spark-analyze-coffee-customers-test:
    name: Spark (analyze-coffee-customers) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-analyze-coffee-customers }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      - name: Install uv
        uses: astral-sh/setup-uv@v6.5.0
        with:
          version: 0.8.13
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/analyze-coffee-customers/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.6.0
        with:
          python-version-file: data-processing/hm-spark/applications/analyze-coffee-customers/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/analyze-coffee-customers
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/analyze-coffee-customers
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.4.3
        with:
          directory: data-processing/hm-spark/applications/analyze-coffee-customers

  spark-find-retired-people-python-test:
    name: Spark (find-retired-people-python) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-find-retired-people-python }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      - name: Install uv
        uses: astral-sh/setup-uv@v6.5.0
        with:
          version: 0.8.13
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/find-retired-people-python/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.6.0
        with:
          python-version-file: data-processing/hm-spark/applications/find-retired-people-python/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/find-retired-people-python
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-retired-people-python
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.4.3
        with:
          directory: data-processing/hm-spark/applications/find-retired-people-python

  spark-find-retired-people-scala-test:
    name: Spark (find-retired-people-scala) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-find-retired-people-scala }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      - name: Set up Java
        uses: actions/setup-java@v4.7.1
        with:
          distribution: corretto
          java-version: '17'
          cache: sbt
      - name: Set up sbt
        uses: sbt/setup-sbt@v1.1.12
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-retired-people-scala
        run: |
          sbt test

  spark-find-taxi-top-routes-test:
    name: Spark (find-taxi-top-routes) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-find-taxi-top-routes }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      - name: Install uv
        uses: astral-sh/setup-uv@v6.5.0
        with:
          version: 0.8.13
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/find-taxi-top-routes/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.6.0
        with:
          python-version-file: data-processing/hm-spark/applications/find-taxi-top-routes/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.4.3
        with:
          directory: data-processing/hm-spark/applications/find-taxi-top-routes

  spark-find-taxi-top-routes-sql-test:
    name: Spark (find-taxi-top-routes-sql) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-find-taxi-top-routes-sql }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      - name: Install uv
        uses: astral-sh/setup-uv@v6.5.0
        with:
          version: 0.8.13
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/find-taxi-top-routes-sql/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.6.0
        with:
          python-version-file: data-processing/hm-spark/applications/find-taxi-top-routes-sql/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes-sql
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes-sql
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.4.3
        with:
          directory: data-processing/hm-spark/applications/find-taxi-top-routes-sql

  spark-ingest-from-s3-to-kafka-test:
    name: Spark (ingest-from-s3-to-kafka) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-ingest-from-s3-to-kafka }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      - name: Set up Java
        uses: actions/setup-java@v4.7.1
        with:
          distribution: corretto
          java-version: '17'
          cache: sbt
      - name: Set up sbt
        uses: sbt/setup-sbt@v1.1.12
      - name: Test
        working-directory: data-processing/hm-spark/applications/ingest-from-s3-to-kafka
        run: |
          sbt test

  spark-recommend-movies-test:
    name: Spark (recommend-movies) | Test
    if: ${{ inputs.workflow || inputs.hm-spark-recommend-movies }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      - name: Install uv
        uses: astral-sh/setup-uv@v6.5.0
        with:
          version: 0.8.13
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/recommend-movies/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.6.0
        with:
          python-version-file: data-processing/hm-spark/applications/recommend-movies/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/recommend-movies
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/recommend-movies
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.4.3
        with:
          directory: data-processing/hm-spark/applications/recommend-movies

  flink-stream-tweets-test:
    name: Flink (stream-tweets) | Test
    if: ${{ inputs.workflow || inputs.flink-stream-tweets }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      - name: Set up Java
        uses: actions/setup-java@v4.7.1
        with:
          distribution: corretto
          java-version: '11'
          cache: maven
      - name: Test
        working-directory: data-processing/flink/applications/stream-tweets
        run: |
          mvn test

  kafka-rust-proto-consumer-test:
    name: Kafka Rust (proto-consumer) | Test
    if: ${{ inputs.workflow || inputs.kafka-rust-proto-consumer }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      # librdkafka is for rdkafka
      - name: Install librdkafka
        env:
          LIBRDKAFKA_VERSION: 2.10.0
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz "https://github.com/edenhill/librdkafka/archive/refs/tags/v${LIBRDKAFKA_VERSION}.tar.gz"
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd "librdkafka-${LIBRDKAFKA_VERSION}"
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f "librdkafka-${LIBRDKAFKA_VERSION}/"
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.14.0
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-consumer
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-consumer
        run: |
          cargo test --all-features

  kafka-rust-proto-producer-test:
    name: Kafka Rust (proto-producer) | Test
    if: ${{ inputs.workflow || inputs.kafka-rust-proto-producer }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      # librdkafka is for rdkafka
      - name: Install librdkafka
        env:
          LIBRDKAFKA_VERSION: 2.10.0
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz "https://github.com/edenhill/librdkafka/archive/refs/tags/v${LIBRDKAFKA_VERSION}.tar.gz"
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd "librdkafka-${LIBRDKAFKA_VERSION}"
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f "librdkafka-${LIBRDKAFKA_VERSION}/"
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.14.0
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-producer
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-producer
        run: |
          cargo test --all-features

  kafka-rust-udp-kafka-bridge-test:
    name: Kafka Rust (udp-kafka-bridge) | Test
    if: ${{ inputs.workflow || inputs.kafka-rust-udp-kafka-bridge }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      # librdkafka is for rdkafka
      - name: Install librdkafka
        env:
          LIBRDKAFKA_VERSION: 2.10.0
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz "https://github.com/edenhill/librdkafka/archive/refs/tags/v${LIBRDKAFKA_VERSION}.tar.gz"
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd "librdkafka-${LIBRDKAFKA_VERSION}"
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f "librdkafka-${LIBRDKAFKA_VERSION}/"
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.14.0
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/udp-kafka-bridge
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/udp-kafka-bridge
        run: |
          cargo test --all-features

  kafka-rust-zeromq-kafka-bridge-test:
    name: Kafka Rust (zeromq-kafka-bridge) | Test
    if: ${{ inputs.workflow || inputs.kafka-rust-zeromq-kafka-bridge }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v5.0.0
      # librdkafka is for rdkafka
      - name: Install librdkafka
        env:
          LIBRDKAFKA_VERSION: 2.10.0
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz "https://github.com/edenhill/librdkafka/archive/refs/tags/v${LIBRDKAFKA_VERSION}.tar.gz"
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd "librdkafka-${LIBRDKAFKA_VERSION}"
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f "librdkafka-${LIBRDKAFKA_VERSION}/"
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.14.0
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/zeromq-kafka-bridge
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/zeromq-kafka-bridge
        run: |
          cargo test --all-features
