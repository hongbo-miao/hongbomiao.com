---
name: . Test Data Processing

'on':
  workflow_call:
    inputs:
      flink-stream-tweets:
        required: true
        type: boolean
      hm-spark-analyze-coffee-customers:
        required: true
        type: boolean
      hm-spark-find-retired-people-python:
        required: true
        type: boolean
      hm-spark-find-retired-people-scala:
        required: true
        type: boolean
      hm-spark-find-taxi-top-routes-sql:
        required: true
        type: boolean
      hm-spark-find-taxi-top-routes:
        required: true
        type: boolean
      hm-spark-ingest-from-s3-to-kafka:
        required: true
        type: boolean
      hm-spark-recommend-movies:
        required: true
        type: boolean
      kafka-rust-proto-consumer:
        required: true
        type: boolean
      kafka-rust-proto-producer:
        required: true
        type: boolean
      kafka-rust-udp-kafka-bridge:
        required: true
        type: boolean
      kafka-rust-zeromq-kafka-bridge:
        required: true
        type: boolean

jobs:
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    permissions:
      pull-requests: read
    outputs:
      workflow: ${{ steps.filter.outputs.workflow }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - uses: dorny/paths-filter@v3.0.2
        id: filter
        with:
          filters: |
            workflow:
              - '.github/workflows/.test-data-processing.yml'

  spark-analyze-coffee-customers-test:
    name: Spark (analyze-coffee-customers) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.hm-spark-analyze-coffee-customers }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Install uv
        uses: astral-sh/setup-uv@v5.2.1
        with:
          version: 0.5.20
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/analyze-coffee-customers/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version-file: data-processing/hm-spark/applications/analyze-coffee-customers/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/analyze-coffee-customers
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/analyze-coffee-customers
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.1.2
        with:
          directory: data-processing/hm-spark/applications/analyze-coffee-customers

  spark-find-retired-people-python-test:
    name: Spark (find-retired-people-python) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.hm-spark-find-retired-people-python }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Install uv
        uses: astral-sh/setup-uv@v5.2.1
        with:
          version: 0.5.20
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/find-retired-people-python/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version-file: data-processing/hm-spark/applications/find-retired-people-python/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/find-retired-people-python
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-retired-people-python
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.1.2
        with:
          directory: data-processing/hm-spark/applications/find-retired-people-python

  spark-find-retired-people-scala-test:
    name: Spark (find-retired-people-scala) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.hm-spark-find-retired-people-scala }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Set up Java
        uses: actions/setup-java@v4.6.0
        with:
          distribution: corretto
          java-version: '17'
          cache: sbt
      - name: Set up sbt
        uses: sbt/setup-sbt@v1.1.5
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-retired-people-scala
        run: |
          sbt test

  spark-find-taxi-top-routes-test:
    name: Spark (find-taxi-top-routes) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.hm-spark-find-taxi-top-routes }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Install uv
        uses: astral-sh/setup-uv@v5.2.1
        with:
          version: 0.5.20
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/find-taxi-top-routes/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version-file: data-processing/hm-spark/applications/find-taxi-top-routes/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.1.2
        with:
          directory: data-processing/hm-spark/applications/find-taxi-top-routes

  spark-find-taxi-top-routes-sql-test:
    name: Spark (find-taxi-top-routes-sql) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.hm-spark-find-taxi-top-routes-sql }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Install uv
        uses: astral-sh/setup-uv@v5.2.1
        with:
          version: 0.5.20
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/find-taxi-top-routes-sql/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version-file: data-processing/hm-spark/applications/find-taxi-top-routes-sql/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes-sql
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/find-taxi-top-routes-sql
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.1.2
        with:
          directory: data-processing/hm-spark/applications/find-taxi-top-routes-sql

  spark-ingest-from-s3-to-kafka-test:
    name: Spark (ingest-from-s3-to-kafka) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.hm-spark-ingest-from-s3-to-kafka }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Set up Java
        uses: actions/setup-java@v4.6.0
        with:
          distribution: corretto
          java-version: '17'
          cache: sbt
      - name: Set up sbt
        uses: sbt/setup-sbt@v1.1.5
      - name: Test
        working-directory: data-processing/hm-spark/applications/ingest-from-s3-to-kafka
        run: |
          sbt test

  spark-recommend-movies-test:
    name: Spark (recommend-movies) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.hm-spark-recommend-movies }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Install uv
        uses: astral-sh/setup-uv@v5.2.1
        with:
          version: 0.5.20
          enable-cache: true
          cache-dependency-glob: data-processing/hm-spark/applications/recommend-movies/uv.lock
      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version-file: data-processing/hm-spark/applications/recommend-movies/pyproject.toml
      - name: Install dependencies
        working-directory: data-processing/hm-spark/applications/recommend-movies
        run: |
          uv sync --dev
      - name: Test
        working-directory: data-processing/hm-spark/applications/recommend-movies
        run: |
          uv run poe test-coverage
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5.1.2
        with:
          directory: data-processing/hm-spark/applications/recommend-movies

  flink-stream-tweets-test:
    name: Flink (stream-tweets) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.flink-stream-tweets }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      - name: Set up Java
        uses: actions/setup-java@v4.6.0
        with:
          distribution: corretto
          java-version: '11'
          cache: maven
      - name: Test
        working-directory: data-processing/flink/applications/stream-tweets
        run: |
          mvn test

  kafka-rust-proto-consumer-test:
    name: Kafka Rust (proto-consumer) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.kafka-rust-proto-consumer }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      # librdkafka is for rdkafka
      - name: Install librdkafka
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz https://github.com/edenhill/librdkafka/archive/refs/tags/v2.8.0.tar.gz
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd librdkafka-2.8.0
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f librdkafka-2.8.0/
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.10.1
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-consumer
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-consumer
        run: |
          cargo test --all-features

  kafka-rust-proto-producer-test:
    name: Kafka Rust (proto-producer) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.kafka-rust-proto-producer }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      # librdkafka is for rdkafka
      - name: Install librdkafka
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz https://github.com/edenhill/librdkafka/archive/refs/tags/v2.8.0.tar.gz
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd librdkafka-2.8.0
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f librdkafka-2.8.0/
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.10.1
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-producer
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/proto-producer
        run: |
          cargo test --all-features

  kafka-rust-udp-kafka-bridge-test:
    name: Kafka Rust (udp-kafka-bridge) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.kafka-rust-udp-kafka-bridge }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      # librdkafka is for rdkafka
      - name: Install librdkafka
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz https://github.com/edenhill/librdkafka/archive/refs/tags/v2.8.0.tar.gz
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd librdkafka-2.8.0
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f librdkafka-2.8.0/
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.10.1
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/udp-kafka-bridge
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/udp-kafka-bridge
        run: |
          cargo test --all-features

  kafka-rust-zeromq-kafka-bridge-test:
    name: Kafka Rust (zeromq-kafka-bridge) | Test
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.workflow == 'true' || inputs.kafka-rust-zeromq-kafka-bridge }}
    runs-on: ubuntu-24.04
    environment: test
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
      # librdkafka is for rdkafka
      - name: Install librdkafka
        run: |
          # https://github.com/confluentinc/librdkafka#build-from-source
          wget --no-verbose --output-document=librdkafka.tar.gz https://github.com/edenhill/librdkafka/archive/refs/tags/v2.8.0.tar.gz
          tar -x -f librdkafka.tar.gz
          rm -f librdkafka.tar.gz
          cd librdkafka-2.8.0
          ./configure
          make
          sudo make install
          sudo ldconfig
          cd ..
          rm -r -f librdkafka-2.8.0/
      # protoc is for prost
      - name: Install protoc
        uses: arduino/setup-protoc@v3.0.0
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          version: 28.3
      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1.10.1
      - name: Install dependencies
        working-directory: data-processing/kafka/kafka-client/kafka-rust/zeromq-kafka-bridge
        run: |
          cargo build
      - name: Test
        working-directory: data-processing/kafka/kafka-client/kafka-rust/zeromq-kafka-bridge
        run: |
          cargo test --all-features
